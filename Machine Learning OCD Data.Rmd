---
title: "Using Machine Learning to Predict BDD in a Cohort of Individuals with OCD"
author: "Jorge Valderrama"
date: "2023-12-26"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("~/Downloads")
OCD <- read.csv ("OCD_BDD_ML3.csv")
```

```{r}
#Checking on structure of data
str(OCD)
```
```{r}
#loading necessary packages
library(tidyverse)
library(caret)
library(party)
```

```{r}
#clean data, check for duplicate files
sapply(OCD, function(x) table(is.na(x)))

table(duplicated(OCD))
OCD <- OCD[!duplicated(OCD),]
```
```{r}
#Visual inspection / descriptive statistics

OCD %>% gather() %>%
  ggplot(aes(x=value)) + 
  geom_histogram(fill="steelblue", alpha=.7) +
  theme_minimal() +
  facet_wrap(~key, scales="free")
```
```{r}
#plotting a correlation matrix, in order to a) check if we have features that are highly correlated (which is problematic for some algorithms), and b) get a first feeling about which features are correlated with the target (MDD) and which are not:
cormatrix <- cor(OCD %>% keep(is.numeric))

cormatrix %>% as.data.frame %>% mutate(var2=rownames(.)) %>%
  pivot_longer(!var2, values_to = "value") %>%
  ggplot(aes(x=name,y=var2,fill=abs(value),label=round(value,2))) +
  theme(axis.text.x=element_text(angle=60, hjust=1))+
  geom_tile() + geom_label() + xlab("") + ylab("") +
  ggtitle("Correlation matrix of our predictors") +
  labs(fill="Correlation\n(absolute):")
```
```{r}
#We can see that aside from the diagonal (correlation of a variable with itself, which is 1), we have no problematically strong correlations between our predictors (strong meaning greater than 0.8 or 0.9 here).

#Taking a look at the bivariate relations between the predictors and the outcome. For continuous predictors and a dichotomous outcome (MDD or no MDD), box plots are a good way of visualizing a bivariate association:

OCD %>% select(-c(Gender,Hoard,Trich,Hosp_OCD,Panic_Disorder, Trauma,PTSD_DX, Panic_Attack, BDD)) %>%
  pivot_longer(!MDD, values_to = "value") %>%
  ggplot(aes(x=factor(MDD), y=value, fill=factor(MDD))) +
  geom_boxplot(outlier.shape = NA) + geom_jitter(size=.7, width=.1, alpha=.5) +
  scale_fill_manual(values=c("steelblue", "orangered1")) +
  labs(fill="MDD:") +
  theme_minimal() +
  facet_wrap(~name, scales="free")
```
```{r}
#For our categorical variables, we use simple stacked barplots to show the differences between individuals with or without MDD:

OCD %>% select(Gender,Hoard,Trich,Hosp_OCD,Panic_Disorder,Trauma,PTSD_DX,Panic_Attack,BDD,MDD) %>% 
  pivot_longer(!MDD, values_to = "value") %>%
  ggplot(aes(x=factor(value), fill=factor(MDD))) +
  scale_fill_manual(values=c("steelblue", "orangered1")) +
  geom_bar(position="fill", alpha=.7)+
  theme_minimal() +
  labs(fill="MDD:") +
  facet_wrap(~name, scales="free")
```
```{r}
#Testing a simple algorithm: “Whenever a patient is female, predict ‘MDD’, otherwise predict ‘no MDD’.” This algorithm would have an accuracy of 60% in our dataset, which you can verify by running:

pred <- as.factor(ifelse(OCD$Gender==1,1,0))
confusionMatrix(pred,as.factor(OCD$MDD))
```
```{r}
#Testing another simple algorithm: “Whenever a patient has a history of at least one traumatic event, predict ‘MDD’, otherwise predict ‘no MDD’.” This algorithm would have an accuracy of 48% in our dataset:

pred <- as.factor(ifelse(OCD$PTSD_DX==1,1,0))
confusionMatrix(pred,as.factor(OCD$MDD))
```
```{r}
#Create a training dataset: create a sequence of random numbers which encompass 70% of our dataset, designate this as “training”, and the rest as a test dataset which will not be touched again until the very end of the analysis:

set.seed(2022)
split <- sample(1:nrow(OCD), as.integer(0.7*nrow(OCD)), F)

train <- OCD[split,]
test <- OCD[-split,]
```

```{r}
#removing MDD as a predictor
preprocess_data <- function(df){  
  #normalizing YBOCS_Score variables with log transformation
  OCD$YBOCS_Score_norm <- log(OCD$YBOCS_SCORE)
  
  return(df[,names(df)!="MDD"])
}
```

```{r}
#Apply the new function to both training and tests datasets. Also vectors y_train and y_test which consist of only the target (MDD 1 or 0)
x_train <- preprocess_data(train)
x_test <- preprocess_data(test)
y_train <- factor(train[,"MDD"], levels=c(1,0))
y_test <- factor(test[,"MDD"], levels=c(1,0))
```

```{r}
#Train a simple decision tree on training data and plot the results.
set.seed(2022)
tree1 <- party::ctree(y_train ~ ., data=cbind(x_train, y_train), 
                      controls = ctree_control(minsplit=10, mincriterion = .9))
plot(tree1)
```
##Random Forest
```{r}
#We use the wrapper function train() from the caret package to train a random forest on our data.

set.seed(2022)
mod <- caret::train(x_train, y_train, method="rf", 
                    tuneGrid = expand.grid(mtry = seq(5,ncol(x_train),by=5)),
                    trControl = trainControl(method="cv", number=5, verboseIter = T))
mod
```
```{r}
#Create a feature importance plot for the random forest model
plot(varImp(mod), main="Feature importance of random forest model on training data")
```
##Neural Network
```{r}
set.seed(2022)
mod2 <- caret::train(x_train, y_train, method="avNNet",
                     preProcess = c("center", "scale", "nzv"),
                     tuneGrid = expand.grid(size = seq(3,21,by=3), decay=c(1e-03, 0.01, 0.1,0),bag=c(T,F)),
                    trControl = trainControl(method="cv", number=5, verboseIter = T),
                    importance=T)
mod2
```

```{r}
#Create a feature importance plot for the neural network model
plot(varImp(mod2), main="Feature importance of neural network classifier on training data")
```
```{r}
#Use the “(extreme) gradient boosted machines” (xgboost) model (works similar to a random forest, except they proceed sequentially: A first tree is grown, then more weight is put on the badly predicted samples before the next tree is grown. As a result, in many cases, xgboost outperforms random forests):

set.seed(2022)
mod3 <- caret::train(x_train, y_train, method="xgbTree", 
                    tuneGrid = expand.grid(nrounds=c(50,100),max_depth=c(5,7,9),
                                           colsample_bytree=c(0.8,1),subsample=c(0.8,1),
                                           min_child_weight=c(1,5,10),eta=c(0.1,0.3),gamma=c(0,0.5)),
                    trControl = trainControl(method="cv", number=5, verboseIter = T))
mod3
```

```{r}
#Create a feature importance plot for the XGBoost model
plot(varImp(mod3), main="Feature importance of XGBoost model on training data")
```
```{r}
#Create a bar graph to demonstrate accuracy in the training data by algorithm
results <- data.frame(Model = c(mod$method,mod2$method, mod3$method),
                      Accuracy = c(max(mod$results$Accuracy), max(mod2$results$Accuracy), max(mod3$results$Accuracy)))
results %>% ggplot(aes(x=Model, y=Accuracy, label=paste(round(100*Accuracy,1),"%"))) +
  geom_col(fill="steelblue") + theme_minimal() + geom_label() +
  ggtitle("Accuracy in the training data by algorithm")
```
##Compare the random forest model's prediction against the reserved test dataset
```{r}
predictions <- predict(mod, newdata = x_test)
confusionMatrix(predictions, y_test)
```
```{r}
##Other metrics to evaluate random forest model
precision(predictions, y_test)
recall(predictions, y_test)
F_meas(predictions, y_test)
```

##Compare the neural network model's prediction against the reserved test dataset
```{r}
predictions <- predict(mod2, newdata = x_test)
confusionMatrix(predictions, y_test)
```
```{r}
##Other metrics to evaluate neural network model
precision(predictions, y_test)
recall(predictions, y_test)
F_meas(predictions, y_test)
```


##Compare the Xgboost model's prediction against the reserved test dataset
```{r}
predictions <- predict(mod3, newdata = x_test)
confusionMatrix(predictions, y_test)
```



```{r}
#Other metrics to evaluate Xgboost model
precision(predictions, y_test)
recall(predictions, y_test)
F_meas(predictions, y_test)
```




